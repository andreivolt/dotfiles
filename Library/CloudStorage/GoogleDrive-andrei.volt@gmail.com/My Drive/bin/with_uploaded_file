#!/usr/bin/env -S uv run --quiet --script
# /// script
# dependencies = [
#    "b2sdk>=1.17.0",
#    "boto3>=1.28.0"
# ]
# ///
import argparse
import atexit
import os
import subprocess
import sys
import tempfile
import boto3
from b2sdk.v2 import B2Api, InMemoryAccountInfo

BUCKET = os.environ.get('BACKBLAZE_BUCKET')
PROFILE = 'backblaze'

parser = argparse.ArgumentParser()
parser.add_argument("-f", "--file", help="File to upload")
args, remaining_args = parser.parse_known_args()

temp_file_path = None
if args.file:
    with open(args.file, 'rb') as f:
        file_content = f.read()
    filename = os.path.basename(args.file)
else:
    # Create a temporary file for the content coming from stdin
    temp_file = tempfile.NamedTemporaryFile(delete=False)
    file_content = sys.stdin.buffer.read()
    temp_file.write(file_content)
    temp_file.close()
    temp_file_path = temp_file.name

    # Use the file command to detect file type
    try:
        file_type = subprocess.check_output(['file', '--brief', '--mime-type', temp_file_path],
                                           universal_newlines=True).strip()

        # Use the subtype as extension
        if '/' in file_type:
            subtype = file_type.split('/')[1]
            if subtype != 'octet-stream':
                ext = '.' + subtype
            else:
                ext = ''
        else:
            ext = ''

        base_name = f"upload_{os.path.basename(temp_file_path)}"
        filename = base_name + ext
    except Exception:
        # Fallback if file command fails
        filename = f"upload_{os.path.basename(temp_file_path)}"

# Get credentials from AWS profile
session = boto3.Session(profile_name=PROFILE)
credentials = session.get_credentials()
aws_access_key_id = credentials.access_key
aws_secret_access_key = credentials.secret_key

if not aws_access_key_id or not aws_secret_access_key:
    print("Error: Couldn't load credentials from AWS profile", file=sys.stderr)
    sys.exit(1)

try:
    # Set up B2 API for upload
    info = InMemoryAccountInfo()
    b2_api = B2Api(info)
    b2_api.authorize_account("production", aws_access_key_id, aws_secret_access_key)

    # Get the bucket
    bucket = b2_api.get_bucket_by_name(BUCKET)

    # Upload file directly from memory using B2 SDK
    uploaded_file = bucket.upload_bytes(
        file_content,
        filename,
        file_info={}
    )

    # Now use boto3 to generate a presigned URL
    s3_client = session.client('s3')

    # Generate the presigned URL with boto3
    download_url = s3_client.generate_presigned_url(
        'get_object',
        Params={'Bucket': BUCKET, 'Key': filename},
        ExpiresIn=3600
    )

    # Output the URL if no command is provided
    if not remaining_args:
        print(download_url)
    # Process the command arguments
    else:
        # Join arguments into a single command string
        cmd_string = ' '.join(remaining_args)
        # Replace placeholder with URL
        cmd_string = cmd_string.replace('{}', download_url)
        # Execute with shell=True to properly handle the command
        subprocess.run(cmd_string, shell=True)

except Exception as e:
    print(f"Error: {str(e)}", file=sys.stderr)
    sys.exit(1)

def cleanup():
    try:
        if 'bucket' in locals() and 'filename' in locals():
            # Use B2 SDK to clean up the file
            file_versions = bucket.list_file_versions(filename)
            for file_version in file_versions:
                bucket.delete_file_version(file_version.id_, filename)
    except Exception as e:
        print(f"Error during cleanup: {str(e)}", file=sys.stderr)

    if temp_file_path:
        try:
            os.unlink(temp_file_path)
        except Exception:
            pass

atexit.register(cleanup)
