#!/usr/bin/env -S uv run --script --quiet
"""Upload file to cloud storage and execute command with uploaded file URL."""
# /// script
# dependencies = [
#   "b2sdk>=1.17",
#   "boto3>=1.28",
#   "sh",
# ]
# ///


import argparse
import atexit
import os
import sys
import tempfile
import shlex
import boto3
import sh
from b2sdk.v2 import B2Api, InMemoryAccountInfo

BUCKET = os.environ.get('BACKBLAZE_BUCKET')
PROFILE = 'backblaze'

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument("-f", "--file", help="File to upload")
args, remaining_args = parser.parse_known_args()

temp_file_path = None
if args.file:
    with open(args.file, 'rb') as f:
        file_content = f.read()
    filename = os.path.basename(args.file)
else:
    # Create a temporary file for the content coming from stdin
    temp_file = tempfile.NamedTemporaryFile(delete=False)
    file_content = sys.stdin.buffer.read()
    temp_file.write(file_content)
    temp_file.close()
    temp_file_path = temp_file.name

    # Use the file command to detect file type
    try:
        file_type = sh.file('--brief', '--mime-type', temp_file_path).strip()

        # Use the subtype as extension
        if '/' in file_type:
            subtype = file_type.split('/')[1]
            if subtype != 'octet-stream':
                ext = '.' + subtype
            else:
                ext = ''
        else:
            ext = ''

        base_name = f"upload_{os.path.basename(temp_file_path)}"
        filename = base_name + ext
    except Exception:
        # Fallback if file command fails
        filename = f"upload_{os.path.basename(temp_file_path)}"

# Get credentials from AWS profile
session = boto3.Session(profile_name=PROFILE)
credentials = session.get_credentials()
aws_access_key_id = credentials.access_key
aws_secret_access_key = credentials.secret_key

if not aws_access_key_id or not aws_secret_access_key:
    print("Error: Couldn't load credentials from AWS profile", file=sys.stderr)
    sys.exit(1)

try:
    # Set up B2 API for upload
    info = InMemoryAccountInfo()
    b2_api = B2Api(info)
    b2_api.authorize_account("production", aws_access_key_id, aws_secret_access_key)

    # Get the bucket
    bucket = b2_api.get_bucket_by_name(BUCKET)

    # Upload file directly from memory using B2 SDK
    uploaded_file = bucket.upload_bytes(
        file_content,
        filename,
        file_info={}
    )

    # Now use boto3 to generate a presigned URL
    s3_client = session.client('s3')

    # Generate the presigned URL with boto3
    download_url = s3_client.generate_presigned_url(
        'get_object',
        Params={'Bucket': BUCKET, 'Key': filename},
        ExpiresIn=3600
    )

    # Process the command arguments if provided
    if remaining_args:
        # Join arguments into a single command string
        cmd_string = ' '.join(remaining_args)
        # If no {} placeholder found, append the URL directly
        if '{}' not in cmd_string:
            cmd_string += ' ' + shlex.quote(download_url)
        else:
            # Replace placeholder with properly escaped URL
            cmd_string = cmd_string.replace('{}', shlex.quote(download_url))
        # Execute using sh and let output pass through
        sh.bash('-c', cmd_string, _fg=True)
    else:
        # Output the URL if no command is provided
        print(download_url)

except Exception as e:
    print(f"Error: {str(e)}", file=sys.stderr)
    sys.exit(1)

def cleanup():
    try:
        if 'bucket' in locals() and 'filename' in locals():
            # Use B2 SDK to clean up the file

            file_versions = bucket.list_file_versions(filename)
            for file_version in file_versions:
                bucket.delete_file_version(file_version.id_, filename)
    except Exception as e:
        print(f"Error during cleanup: {str(e)}", file=sys.stderr)

    if temp_file_path:
        try:
            os.unlink(temp_file_path)
        except Exception:
            pass

atexit.register(cleanup)