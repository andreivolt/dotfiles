#!/usr/bin/env rust-script
//! ```cargo
//! [dependencies]
//! clap = { version = "4", features = ["derive"] }
//! roux = "2"
//! serde = { version = "1", features = ["derive"] }
//! serde_json = "1"
//! chrono = "0.4"
//! url = "2"
//! anyhow = "1"
//! tokio = { version = "1", features = ["rt-multi-thread", "macros"] }
//! dirs = "5"
//! sha2 = "0.10"
//! hex = "0.4"
//! comfy-table = "7.1"
//! scraper = "0.20"
//! ```

use anyhow::{anyhow, Result};
use chrono::{DateTime, Local, TimeZone, Utc};
use clap::Parser;
use comfy_table::{modifiers::UTF8_ROUND_CORNERS, presets::UTF8_FULL, Table};
use roux::Subreddit;
use scraper::{Html, Selector};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use sha2::{Digest, Sha256};
use std::fs;
use std::path::PathBuf;
use std::process::Command;
use url::Url;

#[derive(Parser)]
#[command(about = "Fetch and display Reddit comments in a formatted view")]
struct Args {
    /// The Reddit submission or comment URL
    url: Option<String>,
    
    /// Output the comment tree in JSON format
    #[arg(long)]
    json: bool,
    
    /// Clear the cache and exit
    #[arg(long)]
    clear_cache: bool,
    
    /// Skip cache and fetch fresh data
    #[arg(short = 'x')]
    skip_cache: bool,
}

#[derive(Serialize)]
struct CommentTree {
    id: String,
    author: String,
    time: String,
    text: String,
    children: Vec<CommentTree>,
}

#[derive(Serialize)]
struct CommentJson {
    author: String,
    body: String,
    created_utc: String,
    score: i64,
    replies: Vec<CommentJson>,
}

#[derive(Serialize, Deserialize)]
struct CacheEntry {
    data: Value,
    timestamp: i64,
}

fn get_cache_dir() -> Result<PathBuf> {
    let cache_dir = dirs::cache_dir()
        .ok_or_else(|| anyhow!("Could not determine cache directory"))?
        .join("reddit-comments");
    
    if !cache_dir.exists() {
        fs::create_dir_all(&cache_dir)?;
    }
    
    Ok(cache_dir)
}

fn get_cache_key(submission_id: &str, comment_id: Option<&str>) -> String {
    let mut hasher = Sha256::new();
    hasher.update(submission_id.as_bytes());
    if let Some(cid) = comment_id {
        hasher.update(cid.as_bytes());
    }
    hex::encode(hasher.finalize())
}

fn get_cached_data(submission_id: &str, comment_id: Option<&str>) -> Result<Option<Value>> {
    let cache_dir = get_cache_dir()?;
    let cache_key = get_cache_key(submission_id, comment_id);
    let cache_file = cache_dir.join(format!("{}.json", cache_key));
    
    if !cache_file.exists() {
        return Ok(None);
    }
    
    let cache_content = fs::read_to_string(&cache_file)?;
    let cache_entry: CacheEntry = serde_json::from_str(&cache_content)?;
    
    // Check if cache is less than 1 hour old
    let now = chrono::Utc::now().timestamp();
    if now - cache_entry.timestamp < 3600 {
        Ok(Some(cache_entry.data))
    } else {
        // Cache expired, remove it
        let _ = fs::remove_file(&cache_file);
        Ok(None)
    }
}

fn save_to_cache(submission_id: &str, comment_id: Option<&str>, data: &Value) -> Result<()> {
    let cache_dir = get_cache_dir()?;
    let cache_key = get_cache_key(submission_id, comment_id);
    let cache_file = cache_dir.join(format!("{}.json", cache_key));
    
    let cache_entry = CacheEntry {
        data: data.clone(),
        timestamp: chrono::Utc::now().timestamp(),
    };
    
    let cache_content = serde_json::to_string(&cache_entry)?;
    fs::write(&cache_file, cache_content)?;
    
    Ok(())
}

fn clear_cache() -> Result<()> {
    let cache_dir = get_cache_dir()?;
    
    if cache_dir.exists() {
        for entry in fs::read_dir(&cache_dir)? {
            let entry = entry?;
            if entry.path().extension().map(|s| s == "json").unwrap_or(false) {
                fs::remove_file(entry.path())?;
            }
        }
        println!("Cache cleared successfully");
    } else {
        println!("No cache directory found");
    }
    
    Ok(())
}

fn clean_comment_text(text: &str) -> String {
    // Parse HTML using scraper and extract meaningful content
    let document = Html::parse_fragment(text);
    let mut result = String::new();
    
    
    // Handle tables first
    let table_selector = Selector::parse("table").unwrap();
    for table in document.select(&table_selector) {
        let unicode_table = render_html_table_to_unicode(&table.html());
        result.push_str(&unicode_table);
        result.push('\n');
    }
    
    // Handle ordered lists
    let ol_selector = Selector::parse("ol").unwrap();
    for ol in document.select(&ol_selector) {
        let formatted_list = render_html_list_to_text(&ol.html(), true);
        result.push_str(&formatted_list);
        result.push('\n');
    }
    
    // Handle unordered lists  
    let ul_selector = Selector::parse("ul").unwrap();
    for ul in document.select(&ul_selector) {
        let formatted_list = render_html_list_to_text(&ul.html(), false);
        result.push_str(&formatted_list);
        result.push('\n');
    }
    
    // Handle paragraphs and other text content (but skip those inside lists/tables)
    let p_selector = Selector::parse("p").unwrap();
    for p in document.select(&p_selector) {
        // Skip paragraphs that are inside list items
        let mut is_inside_list = false;
        let mut parent = p.parent();
        while let Some(parent_node) = parent {
            if let Some(parent_element) = parent_node.value().as_element() {
                if parent_element.name() == "li" || parent_element.name() == "ol" || parent_element.name() == "ul" {
                    is_inside_list = true;
                    break;
                }
            }
            parent = parent_node.parent();
        }
        
        if !is_inside_list {
            let text_content = p.text().collect::<Vec<_>>().join(" ").trim().to_string();
            if !text_content.is_empty() {
                result.push_str(&text_content);
                result.push('\n');
            }
        }
    }
    
    // Handle blockquotes
    let blockquote_selector = Selector::parse("blockquote").unwrap();
    for blockquote in document.select(&blockquote_selector) {
        let text_content = blockquote.text().collect::<Vec<_>>().join(" ").trim().to_string();
        if !text_content.is_empty() {
            result.push_str("> ");
            result.push_str(&text_content);
            result.push('\n');
        }
    }
    
    // If no structured content was found, fall back to extracting all text
    if result.trim().is_empty() {
        result = document.root_element().text().collect::<Vec<_>>().join(" ");
    }
    
    // Clean up excessive whitespace while preserving intentional line breaks
    let lines: Vec<&str> = result.lines().collect();
    let mut cleaned_lines = Vec::new();
    
    for line in lines {
        let trimmed = line.trim();
        if !trimmed.is_empty() {
            cleaned_lines.push(trimmed);
        } else if !cleaned_lines.is_empty() && !cleaned_lines.last().unwrap().is_empty() {
            // Preserve single empty lines but not multiple consecutive ones
            cleaned_lines.push("");
        }
    }
    
    // Remove trailing empty lines
    while cleaned_lines.last() == Some(&"") {
        cleaned_lines.pop();
    }
    
    // Remove leading empty lines  
    while cleaned_lines.first() == Some(&"") {
        cleaned_lines.remove(0);
    }
    
    cleaned_lines.join("\n")
}

fn render_html_table_to_unicode(html: &str) -> String {
    let document = Html::parse_fragment(html);
    let tr_selector = Selector::parse("tr").unwrap();
    let td_selector = Selector::parse("td, th").unwrap();
    
    let mut rows: Vec<Vec<String>> = Vec::new();
    
    for tr in document.select(&tr_selector) {
        let mut row = Vec::new();
        for cell in tr.select(&td_selector) {
            row.push(cell.text().collect::<Vec<_>>().join(" ").trim().to_string());
        }
        if !row.is_empty() {
            rows.push(row);
        }
    }
    
    if rows.is_empty() {
        return html.to_string();
    }
    
    // Create unicode table using comfy-table
    let mut table = Table::new();
    table.load_preset(UTF8_FULL);
    table.apply_modifier(UTF8_ROUND_CORNERS);
    
    for (i, row) in rows.iter().enumerate() {
        if i == 0 {
            // First row as header
            table.set_header(row);
        } else {
            table.add_row(row);
        }
    }
    
    table.to_string()
}

fn render_html_list_to_text(html: &str, ordered: bool) -> String {
    let document = Html::parse_fragment(html);
    let li_selector = Selector::parse("li").unwrap();
    
    let mut result = String::new();
    let mut counter = 1;
    
    for li in document.select(&li_selector) {
        let text = li.text().collect::<Vec<_>>().join(" ").trim().to_string();
        if !text.is_empty() {
            if ordered {
                result.push_str(&format!("[{}] {}\n", counter, text));
                counter += 1;
            } else {
                result.push_str(&format!("â€¢ {}\n", text));
            }
        }
    }
    
    result.trim_end().to_string()
}

fn get_submission_and_comment_id(url: &str) -> Result<(String, Option<String>)> {
    let parsed = Url::parse(url)?;
    
    if let Some(host) = parsed.host_str() {
        if host.contains("reddit.com") || host.contains("old.reddit.com") {
            let segments: Vec<&str> = parsed.path_segments()
                .ok_or_else(|| anyhow!("Invalid URL path"))?
                .collect();
            
            // Handle /r/subreddit/comments/submission_id/title/comment_id format
            if segments.len() >= 5 && segments[0] == "r" && segments[2] == "comments" {
                let submission_id = segments[3].to_string();
                let comment_id = if segments.len() >= 6 && !segments[5].is_empty() {
                    Some(segments[5].to_string())
                } else {
                    None
                };
                return Ok((submission_id, comment_id));
            }
        }
    }
    
    Err(anyhow!("Invalid Reddit URL"))
}

fn parse_comment_to_tree(comment: &Value) -> Option<CommentTree> {
    let data = comment.get("data")?;
    
    // Skip deleted/removed/empty comments
    let author = data.get("author")?.as_str()?;
    if author == "[deleted]" || author == "None" {
        return None;
    }
    
    // Use body_html field which contains the actual HTML content
    let body = if let Some(body_html) = data.get("body_html").and_then(|v| v.as_str()) {
        // Decode HTML entities from the body_html field
        body_html
            .replace("&lt;", "<")
            .replace("&gt;", ">")
            .replace("&amp;", "&")
            .replace("&quot;", "\"")
            .replace("&#39;", "'")
    } else {
        // Fallback to regular body field
        data.get("body")?.as_str()?.to_string()
    };
    
    if body.is_empty() || body == "[deleted]" || body == "[removed]" {
        return None;
    }
    
    let id = data.get("id")?.as_str()?;
    let created_utc = data.get("created_utc")?.as_f64()?;
    let timestamp = Utc.timestamp_opt(created_utc as i64, 0).single()?;
    let local_time = DateTime::<Local>::from(timestamp);
    
    let mut children = Vec::new();
    if let Some(replies) = data.get("replies") {
        if let Some(replies_obj) = replies.as_object() {
            if let Some(listing_data) = replies_obj.get("data") {
                if let Some(listing_children) = listing_data.get("children") {
                    if let Some(listing_array) = listing_children.as_array() {
                        for reply in listing_array {
                            if reply.get("kind").and_then(|k| k.as_str()) == Some("t1") {
                                if let Some(child) = parse_comment_to_tree(reply) {
                                    children.push(child);
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    
    let cleaned_text = clean_comment_text(&body);
    
    
    Some(CommentTree {
        id: id.to_string(),
        author: author.to_string(),
        time: local_time.format("%Y-%m-%d %H:%M").to_string(),
        text: cleaned_text,
        children,
    })
}

fn parse_comment_to_json(comment: &Value) -> CommentJson {
    let data = &comment["data"];
    
    let author = data.get("author")
        .and_then(|a| a.as_str())
        .unwrap_or("[deleted]")
        .to_string();
    // Use body_html field which contains the actual HTML content
    let body = if let Some(body_html) = data.get("body_html").and_then(|v| v.as_str()) {
        // Decode HTML entities from the body_html field
        body_html
            .replace("&lt;", "<")
            .replace("&gt;", ">")
            .replace("&amp;", "&")
            .replace("&quot;", "\"")
            .replace("&#39;", "'")
    } else {
        // Fallback to regular body field
        data.get("body")
            .and_then(|b| b.as_str())
            .unwrap_or("")
            .to_string()
    };
    let score = data.get("score")
        .and_then(|s| s.as_i64())
        .unwrap_or(0);
    let created_utc = data.get("created_utc")
        .and_then(|c| c.as_f64())
        .unwrap_or(0.0);
    let timestamp = Utc.timestamp_opt(created_utc as i64, 0).single()
        .unwrap_or_else(Utc::now);
    
    let mut replies = Vec::new();
    if let Some(replies_val) = data.get("replies") {
        if let Some(replies_obj) = replies_val.as_object() {
            if let Some(listing_data) = replies_obj.get("data") {
                if let Some(listing_children) = listing_data.get("children") {
                    if let Some(listing_array) = listing_children.as_array() {
                        for reply in listing_array {
                            if reply.get("kind").and_then(|k| k.as_str()) == Some("t1") {
                                replies.push(parse_comment_to_json(reply));
                            }
                        }
                    }
                }
            }
        }
    }
    
    CommentJson {
        author,
        body: clean_comment_text(&body),
        created_utc: timestamp.to_rfc3339(),
        score,
        replies,
    }
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    
    // Handle clear cache command
    if args.clear_cache {
        return clear_cache();
    }
    
    let url = args.url.ok_or_else(|| anyhow!("URL is required"))?;
    let (submission_id, comment_id) = get_submission_and_comment_id(&url)?;
    
    // Try to get from cache first (unless skip_cache is set)
    let comments_value = if args.skip_cache {
        None
    } else {
        get_cached_data(&submission_id, comment_id.as_deref())?
    };
    
    let comments_value = if let Some(cached_data) = comments_value {
        cached_data
    } else {
        // Fetch fresh data
        let subreddit = Subreddit::new("all");
        let comments_response = subreddit.article_comments(&submission_id, None, Some(500)).await?;
        let comments_value: Value = serde_json::to_value(&comments_response)?;
        
        // Save to cache
        if let Err(e) = save_to_cache(&submission_id, comment_id.as_deref(), &comments_value) {
            eprintln!("Warning: Failed to save to cache: {}", e);
        }
        
        comments_value
    };
    
    // The roux response structure is: { data: { children: [...] } }
    let comments = if let Some(cid) = comment_id {
        // Find specific comment in the tree
        let mut found_comments = Vec::new();
        
        // Check if comments_value has the expected structure
        if let Some(data) = comments_value.get("data") {
            if let Some(children) = data.get("children").and_then(|c| c.as_array()) {
                for comment in children {
                    if comment.get("data").and_then(|d| d.get("id")).and_then(|id| id.as_str()) == Some(&cid) {
                        found_comments.push(comment.clone());
                        break;
                    }
                }
            }
        }
        
        if found_comments.is_empty() {
            return Err(anyhow!("Comment not found"));
        }
        found_comments
    } else {
        // Get all comments - handle roux response structure
        if let Some(data) = comments_value.get("data") {
            if let Some(children) = data.get("children").and_then(|c| c.as_array()) {
                children.iter()
                    .filter(|c| c.get("kind").and_then(|k| k.as_str()) == Some("t1"))
                    .cloned()
                    .collect()
            } else {
                Vec::new()
            }
        } else {
            Vec::new()
        }
    };
    
    if args.json {
        let comment_tree: Vec<CommentJson> = comments.iter()
            .map(parse_comment_to_json)
            .collect();
        println!("{}", serde_json::to_string(&comment_tree)?);
    } else {
        let tree_comments: Vec<CommentTree> = comments.iter()
            .filter_map(parse_comment_to_tree)
            .collect();
        
        let json_input = serde_json::to_string(&tree_comments)?;
        
        let mut child = Command::new("tree-render")
            .arg("--author=author")
            .arg("--timestamp=time")
            .arg("--content=text")
            .arg("--replies=children")
            .stdin(std::process::Stdio::piped())
            .stdout(std::process::Stdio::inherit())
            .stderr(std::process::Stdio::inherit())
            .spawn()?;
        
        if let Some(mut stdin) = child.stdin.take() {
            use std::io::Write;
            stdin.write_all(json_input.as_bytes())?;
        }
        
        child.wait()?;
    }
    
    Ok(())
}