#!/usr/bin/env -S uv run --script --quiet
"""Broadcast audio to Chromecast devices."""
# /// script
# dependencies = [
#   "flask>=3.0",
#   "pyaudio>=0.2.14",
#   "pychromecast>=14.0",
#   "sh>=2.0",
# ]
# ///


import pychromecast
import sys
import time
import tempfile
import os
import json
import argparse
import threading
import socket
import signal
import subprocess
from pathlib import Path
from flask import Flask, send_file, request

CACHE_FILE = Path.home() / '.cache' / 'castpipe_chromecasts.json'
CACHE_FILE.parent.mkdir(exist_ok=True)

def get_chromecasts(force_scan=False):
    """Get Chromecasts, using cache if available."""
    if not force_scan and CACHE_FILE.exists():
        try:
            with open(CACHE_FILE) as f:
                cached = json.load(f)
            # Check if cache is less than 1 hour old
            if time.time() - cached['timestamp'] < 3600:
                return cached['devices'], None
        except:
            pass

    # Scan for devices
    print("Scanning for Chromecasts...")
    chromecasts, browser = pychromecast.discovery.discover_chromecasts()

    # Cache the results
    devices = [{'name': cc.friendly_name, 'host': cc.host, 'port': cc.port, 'uuid': str(cc.uuid)}
               for cc in chromecasts]

    with open(CACHE_FILE, 'w') as f:
        json.dump({'timestamp': time.time(), 'devices': devices}, f)

    return devices, browser

def find_chromecast(name, devices, browser=None):
    """Find Chromecast by exact name (case-insensitive)."""
    name_lower = name.lower()
    target_device = None
    for device in devices:
        if device['name'].lower() == name_lower:
            target_device = device
            break

    if not target_device:
        return None if browser else (None, None)

    # Use the specific host/port to connect to the right device
    if browser:
        # Get all chromecasts and find the one matching our target
        chromecasts, _ = pychromecast.discovery.discover_chromecasts()
        for cc in chromecasts:
            if cc.uuid == target_device['uuid'] or (cc.host == target_device['host'] and cc.port == target_device['port']):
                return pychromecast.get_chromecast_from_cast_info(cc, browser.zc)
        return None
    else:
        # Discover and connect to specific device
        chromecasts, new_browser = pychromecast.discovery.discover_chromecasts()
        for cc in chromecasts:
            if cc.uuid == target_device['uuid'] or (cc.host == target_device['host'] and cc.port == target_device['port']):
                cast = pychromecast.get_chromecast_from_cast_info(cc, new_browser.zc)
                return cast, new_browser
        return None, None

def convert_to_mp3(audio_data):
    """Convert audio to MP3 for better compatibility."""
    # Save input data
    if audio_data[:4] == b'FORM':
        print("Converting AIFF to MP3...")
        with tempfile.NamedTemporaryFile(delete=False, suffix='.aiff') as tmp_aiff:
            tmp_aiff.write(audio_data)
            input_file = tmp_aiff.name
    elif audio_data[:4] == b'RIFF':
        print("Converting WAV to MP3...")
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_wav:
            tmp_wav.write(audio_data)
            input_file = tmp_wav.name
    elif audio_data[:4] == b'\x1a\x45\xdf\xa3' or audio_data[:3] == b'ID3':
        # WebM/Matroska or MP3
        print("Converting WebM/audio to MP3...")
        with tempfile.NamedTemporaryFile(delete=False, suffix='.webm') as tmp_webm:
            tmp_webm.write(audio_data)
            input_file = tmp_webm.name
    else:
        # Try as generic audio
        print("Converting unknown audio format to MP3...")
        with tempfile.NamedTemporaryFile(delete=False, suffix='.audio') as tmp:
            tmp.write(audio_data)
            input_file = tmp.name

    # Convert to MP3
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_mp3:
        audio_file = tmp_mp3.name

    # Use sh for better error handling
    import sh
    # Higher quality conversion with volume normalization
    try:
        sh.ffmpeg(
            '-i', input_file,
            '-acodec', 'libmp3lame',
            '-ar', '48000',
            '-ab', '320k',
            '-ac', '2',
            '-af', 'loudnorm=I=-16:TP=-1.5:LRA=11',
            '-y', audio_file
        )
    except sh.ErrorReturnCode as e:
        print(f"Warning: ffmpeg conversion failed with code {e.exit_code}")
        print(f"Error: {str(e.stderr)[:500]}")  # Show first 500 chars of error
        # Try a simpler conversion with just volume boost
        try:
            sh.ffmpeg('-i', input_file, '-af', 'volume=2.0', '-y', audio_file)
        except sh.ErrorReturnCode:
            print("Simple conversion also failed")
            # Last resort - just copy
            try:
                sh.ffmpeg('-i', input_file, '-acodec', 'copy', '-y', audio_file)
            except sh.ErrorReturnCode:
                pass

    os.unlink(input_file)
    return audio_file

def stream_audio_live(cast, cast_name, port=None):
    """Stream live audio directly to Chromecast."""
    import subprocess
    import sh

    print("Starting live audio stream... Press Ctrl+C to stop")

    # Find an available port if not specified
    if port is None:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))
            port = s.getsockname()[1]

    print(f"Using port {port} for live stream")

    # Get local IP
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    s.connect(("8.8.8.8", 80))
    local_ip = s.getsockname()[0]
    s.close()

    # Start HTTP server for streaming
    app = Flask(__name__)

    # Global variable to store the ffmpeg process
    ffmpeg_proc = None

    @app.route('/stream.mp3')
    def stream_audio():
        nonlocal ffmpeg_proc
        print(f"Stream request from {request.remote_addr}")

        def generate():
            nonlocal ffmpeg_proc
            try:
                # Start ffmpeg process to capture and encode audio
                ffmpeg_proc = subprocess.Popen([
                    'ffmpeg',
                    '-f', 'avfoundation',
                    '-i', ':0',  # Default audio input
                    '-acodec', 'libmp3lame',
                    '-ar', '44100',
                    '-ab', '128k',
                    '-ac', '2',
                    '-f', 'mp3',
                    '-'
                ], stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)

                # Stream the output
                while True:
                    data = ffmpeg_proc.stdout.read(4096)
                    if not data:
                        break
                    yield data

            except Exception as e:
                print(f"Stream error: {e}")
            finally:
                if ffmpeg_proc:
                    ffmpeg_proc.terminate()
                    ffmpeg_proc = None

        return app.response_class(generate(), mimetype='audio/mpeg')

    # Start server in background
    def run_server():
        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False, threaded=True)

    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    time.sleep(2)  # Give server time to start

    # Connect to Chromecast and start playback
    print(f"Connecting to {cast_name} at {cast.cast_info.host}:{cast.cast_info.port}...")
    cast.wait()

    mc = cast.media_controller
    stream_url = f'http://{local_ip}:{port}/stream.mp3'
    print(f"Starting live stream: {stream_url}")

    mc.play_media(stream_url, 'audio/mpeg')
    mc.block_until_active()

    print(f"Live streaming to {cast_name}")
    print("Press Ctrl+C to stop streaming")

    try:
        # Keep streaming until interrupted
        while True:
            time.sleep(1)
            # Check if still playing
            if mc.status.player_state == 'IDLE':
                print("Playback stopped, restarting stream...")
                mc.play_media(stream_url, 'audio/mpeg')
                mc.block_until_active()
    except KeyboardInterrupt:
        print("\nStopping live stream...")
        if ffmpeg_proc:
            ffmpeg_proc.terminate()
        mc.stop()
        cast.quit_app()

def record_audio():
    """Record audio from microphone until Ctrl+C."""
    print("Recording... Press Ctrl+C to stop and cast")

    # Record to WAV first (sox doesn't support direct MP3 encoding)
    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
        wav_file = tmp.name

    # Use sh for recording commands
    import sh

    # Set up signal handler to catch Ctrl+C
    proc = None

    def signal_handler(sig, frame):
        if proc:
            proc.terminate()

    signal.signal(signal.SIGINT, signal_handler)

    try:
        # Try rec first (sox)
        try:
            proc = sh.rec(wav_file, _bg=True)
            proc.wait()
        except sh.CommandNotFound:
            # Fallback to ffmpeg
            try:
                proc = sh.ffmpeg(
                    '-f', 'avfoundation', '-i', ':0',
                    '-acodec', 'pcm_s16le',
                    '-ar', '44100', '-ac', '2', '-y', wav_file,
                    _bg=True, _err_to_out=True
                )
                proc.wait()
            except sh.CommandNotFound:
                print("Error: Could not find 'rec' (sox) or 'ffmpeg' for recording")
                sys.exit(1)
    except KeyboardInterrupt:
        print("\nRecording stopped.")

    # Reset signal handler
    signal.signal(signal.SIGINT, signal.default_int_handler)

    # Check if file was created and has content
    if not os.path.exists(wav_file) or os.path.getsize(wav_file) == 0:
        print("No audio recorded")
        sys.exit(1)

    # Convert WAV to MP3
    print("Converting to MP3...")
    with open(wav_file, 'rb') as f:
        wav_data = f.read()

    mp3_file = convert_to_mp3(wav_data)
    os.unlink(wav_file)

    return mp3_file

def play_to_chromecast(audio_file, cast, cast_name, port=None, stop_event=None):
    """Play audio file to Chromecast."""
    # Check file size
    file_size = os.path.getsize(audio_file)
    print(f"Audio file size: {file_size} bytes")

    # Find an available port if not specified
    if port is None:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))
            port = s.getsockname()[1]

    print(f"Using port {port} for audio server")

    # Simple HTTP server to serve the audio file
    app = Flask(__name__)

    @app.route('/audio.mp3')
    def serve_audio():
        print(f"HTTP request received from {request.remote_addr}")
        if os.path.exists(audio_file):
            print(f"Serving audio file: {audio_file} ({os.path.getsize(audio_file)} bytes)")
            return send_file(audio_file, mimetype='audio/mpeg', as_attachment=False)
        else:
            print(f"Audio file not found: {audio_file}")
            return "File not found", 404

    # Start server in background
    def run_server():
        app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)

    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    time.sleep(2)  # Give server more time to start

    # Get local IP
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    s.connect(("8.8.8.8", 80))
    local_ip = s.getsockname()[0]
    s.close()

    # Cast the audio
    print(f"Connecting to {cast_name} at {cast.cast_info.host}:{cast.cast_info.port}...")
    cast.wait()

    mc = cast.media_controller
    audio_url = f'http://{local_ip}:{port}/audio.mp3'
    print(f"Streaming from: {audio_url}")

    mc.play_media(audio_url, 'audio/mpeg')
    mc.block_until_active()

    print(f"Playing on {cast_name}")

    # Check for stop signal before continuing
    if stop_event and stop_event.is_set():
        print("Stopping due to cancellation")
        cast.quit_app()
        return

    # Give extra time for Kitchen speaker which seems slower
    if "kitchen" in cast_name.lower():
        time.sleep(5)
    else:
        time.sleep(2)

    try:
        # Wait for playback to complete or stop signal
        wait_count = 0
        while mc.status.player_state != 'IDLE' and wait_count < 120:  # Max 60 seconds
            if stop_event and stop_event.is_set():
                print("Stopping playback due to cancellation")
                mc.stop()
                break
            time.sleep(0.5)
            wait_count += 1
    except KeyboardInterrupt:
        print("\nStopping...")

    # Give a moment for any final HTTP requests
    time.sleep(1)

    cast.quit_app()

def run_web_interface():
    """Run the web interface."""
    from flask import Flask, render_template_string, jsonify, request as flask_request
    import base64
    import threading

    # Global stop event for cancelling playback
    current_stop_event = None
    current_stop_lock = threading.Lock()

    app = Flask(__name__)

    # HTML template with modern UI
    HTML_TEMPLATE = '''
    <!DOCTYPE html>
    <html>
    <head>
        <title></title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta charset="UTF-8">
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            @keyframes breathe {
                0%, 100% { transform: scale(1); }
                50% { transform: scale(1.05); }
            }
            .breathing {
                animation: breathe 2s ease-in-out infinite;
            }
            @keyframes processing {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
            .processing-spinner {
                animation: processing 1s linear infinite;
            }
        </style>
    </head>
    <body class="bg-gray-50 min-h-screen flex items-center justify-center">
        <div class="w-full max-w-md">
            <!-- Speaker List -->
            <div class="p-4 grid grid-cols-2 gap-3" id="speakerButtons">
                <!-- Speaker buttons will be added here -->
            </div>

            <!-- Record Button -->
            <div class="p-6 flex justify-center">
                <button id="recordBtn" onclick="toggleRecording()"
                    class="w-16 h-16 rounded-full bg-blue-500 hover:bg-blue-600 text-white transition-all duration-300 flex items-center justify-center shadow-lg hover:shadow-xl disabled:opacity-50 disabled:cursor-not-allowed relative overflow-hidden group">
                    <svg id="micIcon" class="w-6 h-6 transition-all duration-300" fill="currentColor" viewBox="0 0 24 24">
                        <path d="M12 14a3 3 0 0 0 3-3V6a3 3 0 0 0-6 0v5a3 3 0 0 0 3 3z"/>
                        <path d="M16.5 11c0 2.485-2.015 4.5-4.5 4.5S7.5 13.485 7.5 11H6c0 2.933 2.164 5.363 5 5.82V19h-2v1.5h6V19h-2v-2.18c2.836-.457 5-2.887 5-5.82h-1.5z"/>
                    </svg>
                    <svg id="stopIcon" class="w-6 h-6 transition-all duration-300 absolute opacity-0" fill="currentColor" viewBox="0 0 24 24">
                        <rect x="6" y="6" width="12" height="12" rx="2"/>
                    </svg>
                </button>
            </div>
        </div>

        <script>
            let isRecording = false;
            let mediaRecorder;
            let audioChunks = [];
            let selectedSpeaker = null;

            // Speaker name mapping
            const speakerMapping = {};

            // Load speakers on page load
            async function loadSpeakers() {
                try {
                    const response = await fetch('/api/speakers');
                    const speakers = await response.json();

                    const container = document.getElementById('speakerButtons');
                    container.innerHTML = '';

                    // Create grid of speaker buttons
                    speakers.forEach((speaker, index) => {
                        const displayName = speaker.name;
                        const button = document.createElement('button');

                        // Room emoji mapping
                        const roomEmojis = {};

                        const emoji = roomEmojis[displayName] || '🔊';

                        button.className = `p-4 rounded-lg transition-all duration-200
                            flex flex-col items-center justify-center
                            text-gray-700 hover:bg-gray-50
                            border-2 ${selectedSpeaker === speaker.name
                                ? 'border-blue-500 bg-blue-50'
                                : 'border-gray-200 bg-white'}
                            shadow-sm hover:shadow-md aspect-square`;

                        button.innerHTML = `
                            <div class="text-sm font-medium text-center">${displayName}</div>
                        `;

                        button.onclick = () => selectSpeaker(speaker.name, button);
                        container.appendChild(button);
                    });

                    // Restore last selection
                    const lastSpeaker = localStorage.getItem('lastSpeaker');
                    if (lastSpeaker && speakers.find(s => s.name === lastSpeaker)) {
                        selectSpeaker(lastSpeaker);
                    }
                } catch (error) {
                    showStatus('Error loading speakers', 'error');
                }
            }

            function selectSpeaker(speakerName, buttonElement) {
                selectedSpeaker = speakerName;
                localStorage.setItem('lastSpeaker', speakerName);

                // Update all buttons to show selection state
                document.querySelectorAll('#speakerButtons button').forEach(btn => {
                    const isSelected = btn === buttonElement;

                    // Update button styling
                    if (isSelected) {
                        btn.className = btn.className.replace('border-gray-200 bg-white', 'border-blue-500 bg-blue-50');
                    } else {
                        btn.className = btn.className.replace('border-blue-500 bg-blue-50', 'border-gray-200 bg-white');
                    }
                });

                // Button selection handled by styling only
            }

            async function toggleRecording() {
                if (!selectedSpeaker) {
                    showStatus('Please select a speaker first', 'error');
                    return;
                }

                // If currently sending, cancel the request
                if (currentRequest) {
                    currentRequest.abort();
                    currentRequest = null;

                    // Also send cancel request to server
                    fetch('/api/cancel', { method: 'POST' })
                        .catch(err => console.log('Cancel request failed:', err));

                    resetToIdle();
                    return;
                }

                if (!isRecording) {
                    await startRecording();
                } else {
                    await stopRecording();
                }
            }

            function resetToIdle() {
                const btn = document.getElementById('recordBtn');
                const micIcon = document.getElementById('micIcon');
                const stopIcon = document.getElementById('stopIcon');

                btn.className = 'w-16 h-16 rounded-full bg-blue-500 hover:bg-blue-600 text-white transition-all duration-300 flex items-center justify-center shadow-lg hover:shadow-xl disabled:opacity-50 disabled:cursor-not-allowed relative overflow-hidden group';
                btn.disabled = false;
                btn.title = '';

                micIcon.style.opacity = '1';
                micIcon.style.transform = 'scale(1) rotate(0deg)';
                stopIcon.style.opacity = '0';
            }

            async function startRecording() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: false,
                            noiseSuppression: false,
                            autoGainControl: false
                        }
                    });

                    // Simple MediaRecorder without specific options
                    mediaRecorder = new MediaRecorder(stream);
                    audioChunks = [];

                    mediaRecorder.ondataavailable = (event) => {
                        if (event.data && event.data.size > 0) {
                            audioChunks.push(event.data);
                            console.log('Chunk received:', event.data.size, 'bytes');
                        }
                    };

                    mediaRecorder.onstop = async () => {
                        console.log('Recording stopped. Total chunks:', audioChunks.length);
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        console.log('Audio blob size:', audioBlob.size, 'bytes');
                        if (audioBlob.size === 0) {
                            const btn = document.getElementById('recordBtn');

                            // Error state for empty audio
                            btn.className = 'w-16 h-16 rounded-full bg-red-400 text-white transition-all duration-300 flex items-center justify-center shadow-lg relative overflow-hidden';

                            setTimeout(() => {
                                resetToIdle();
                            }, 600);
                            return;
                        }
                        await sendAudioToChromecast(audioBlob);
                    };

                    mediaRecorder.start(1000); // Request data every second
                    isRecording = true;

                    const btn = document.getElementById('recordBtn');

                    // Transition to recording state
                    btn.className = 'w-16 h-16 rounded-full bg-red-500 hover:bg-red-600 text-white transition-all duration-300 flex items-center justify-center shadow-lg hover:shadow-xl relative overflow-hidden breathing';

                    // Icon transition
                    const micIcon = document.getElementById('micIcon');
                    const stopIcon = document.getElementById('stopIcon');
                    micIcon.style.opacity = '0';
                    micIcon.style.transform = 'scale(0.8) rotate(-90deg)';
                    stopIcon.style.opacity = '1';
                    stopIcon.style.transform = 'scale(1) rotate(0deg)';
                } catch (error) {
                    showStatus('Error accessing microphone', 'error');
                }
            }

            async function stopRecording() {
                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                    mediaRecorder.stream.getTracks().forEach(track => track.stop());

                    isRecording = false;
                    const btn = document.getElementById('recordBtn');

                    // Transition to processing state
                    btn.className = 'w-16 h-16 rounded-full bg-orange-500 hover:bg-orange-600 text-white transition-all duration-300 flex items-center justify-center shadow-lg hover:shadow-xl relative overflow-hidden';
                    btn.disabled = false;
                    btn.title = 'Click to cancel sending';
                }
            }

            let currentRequest = null;

            async function sendAudioToChromecast(audioBlob) {
                try {
                    const reader = new FileReader();
                    reader.onloadend = async () => {
                        const base64Audio = reader.result.split(',')[1];

                        const controller = new AbortController();
                        currentRequest = controller;

                        const response = await fetch('/api/cast', {
                            method: 'POST',
                            headers: { 'Content-Type': 'application/json' },
                            body: JSON.stringify({
                                audio: base64Audio,
                                speaker: selectedSpeaker
                            }),
                            signal: controller.signal
                        });

                        const result = await response.json();

                        const btn = document.getElementById('recordBtn');

                        // Processing complete

                        currentRequest = null;

                        if (result.success) {
                            // Success state
                            btn.className = 'w-16 h-16 rounded-full bg-green-500 text-white transition-all duration-300 flex items-center justify-center shadow-lg relative overflow-hidden';

                            // Complete the transition after a brief moment
                            setTimeout(() => {
                                resetToIdle();
                            }, 600);
                        } else {
                            // Error state
                            btn.className = 'w-16 h-16 rounded-full bg-red-400 text-white transition-all duration-300 flex items-center justify-center shadow-lg relative overflow-hidden';

                            setTimeout(() => {
                                resetToIdle();
                            }, 600);
                        }
                    };
                    reader.readAsDataURL(audioBlob);
                } catch (error) {
                    currentRequest = null;

                    // Check if it was an abort (user cancellation)
                    if (error.name === 'AbortError') {
                        resetToIdle();
                        return;
                    }

                    const btn = document.getElementById('recordBtn');

                    // Error state
                    btn.className = 'w-16 h-16 rounded-full bg-red-400 text-white transition-all duration-300 flex items-center justify-center shadow-lg relative overflow-hidden';

                    setTimeout(() => {
                        resetToIdle();
                    }, 600);
                }
            }

            function showStatus(message, type) {
                // Status messages removed for cleaner interface
                // Visual feedback is now handled by button color changes
                console.log(`${type}: ${message}`);
            }

            // Load speakers when page loads
            loadSpeakers();

            // Refresh speakers every 30 seconds
            setInterval(loadSpeakers, 30000);
        </script>
    </body>
    </html>
    '''

    @app.route('/')
    def index():
        return render_template_string(HTML_TEMPLATE)

    @app.route('/api/speakers')
    def api_speakers():
        devices, browser = get_chromecasts()
        if browser:
            pychromecast.discovery.stop_discovery(browser)
        return jsonify(devices)

    @app.route('/api/cast', methods=['POST'])
    def api_cast():
        try:
            data = flask_request.json
            audio_b64 = data['audio']
            speaker_name = data['speaker']

            # Decode base64 audio
            audio_data = base64.b64decode(audio_b64)

            # Debug: Check what format we received
            print(f"Received audio data: {len(audio_data)} bytes")
            if len(audio_data) > 0:
                print(f"First 16 bytes: {audio_data[:16].hex()}")
            else:
                print("ERROR: Received empty audio data!")

            # Convert to MP3
            audio_file = convert_to_mp3(audio_data)

            # Find and cast to speaker
            devices, browser = get_chromecasts()

            if browser:
                cast = find_chromecast(speaker_name, devices, browser)
                if not cast:
                    return jsonify({'success': False, 'error': 'Speaker not found'})
            else:
                cast, browser = find_chromecast(speaker_name, devices)
                if not cast:
                    return jsonify({'success': False, 'error': 'Speaker not found'})

            # Create stop event for this playback
            with current_stop_lock:
                global current_stop_event
                current_stop_event = threading.Event()
                stop_event = current_stop_event

            # Play the audio (use random port for web mode)
            play_to_chromecast(audio_file, cast, speaker_name, stop_event=stop_event)

            # Clear the stop event when done
            with current_stop_lock:
                if current_stop_event == stop_event:
                    current_stop_event = None

            # Cleanup browser but NOT the audio file yet - it's still being served
            if browser:
                pychromecast.discovery.stop_discovery(browser)

            # Schedule file deletion after a delay
            def cleanup_file():
                time.sleep(30)  # Give Chromecast time to fetch the file
                if os.path.exists(audio_file):
                    os.unlink(audio_file)
                    print(f"Cleaned up audio file: {audio_file}")

            cleanup_thread = threading.Thread(target=cleanup_file, daemon=True)
            cleanup_thread.start()

            return jsonify({'success': True})

        except Exception as e:
            return jsonify({'success': False, 'error': str(e)})

    @app.route('/api/cancel', methods=['POST'])
    def api_cancel():
        try:
            with current_stop_lock:
                global current_stop_event
                if current_stop_event:
                    current_stop_event.set()
                    print("Cancellation requested")
                    return jsonify({'success': True})
                else:
                    return jsonify({'success': False, 'error': 'No active playback to cancel'})
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)})

    print("Starting Castpipe web interface on http://localhost:8080")
    app.run(host='0.0.0.0', port=8080, debug=False)

# Parse arguments
parser = argparse.ArgumentParser(description=__doc__.strip(), formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('audio_file', nargs='?', help='Audio file to play (optional if piping)')
parser.add_argument('-s', '--speaker', default='', help='Speaker name (exact match, e.g., "Bedroom speaker" or "Kitchen speaker")')
parser.add_argument('-l', '--list', action='store_true', help='List available Chromecasts')
parser.add_argument('--scan', action='store_true', help='Force rescan for Chromecasts')
parser.add_argument('--live', action='store_true', help='Stream live (requires ffmpeg with icecast support)')
parser.add_argument('--web', action='store_true', help='Run web interface on port 8080')

args = parser.parse_args()

# Web interface mode
if args.web:
    run_web_interface()
    sys.exit(0)

# List devices
if args.list or (args.scan and not args.audio_file and not args.speaker and sys.stdin.isatty()):
    devices, browser = get_chromecasts(args.scan)
    print("Available Chromecasts:")
    for device in devices:
        print(f"  - {device['name']} ({device['host']}:{device['port']})")
    if browser:
        pychromecast.discovery.stop_discovery(browser)
    sys.exit(0)

# Determine audio source
audio_file = None
temp_file = False
browser = None

try:
    if args.audio_file:
        # File provided
        with open(args.audio_file, 'rb') as f:
            audio_data = f.read()
        audio_file = convert_to_mp3(audio_data)
        temp_file = True
    elif not sys.stdin.isatty():
        # Piped input
        audio_data = sys.stdin.buffer.read()
        audio_file = convert_to_mp3(audio_data)
        temp_file = True
    else:
        # Always use live streaming for microphone input
        pass  # We'll handle this after getting the cast device

    # Get Chromecasts
    devices, browser = get_chromecasts(args.scan)

    # Find target Chromecast
    if args.speaker:
        if browser:
            cast = find_chromecast(args.speaker, devices, browser)
            if not cast:
                print(f"Chromecast '{args.speaker}' not found. Available devices:")
                for device in devices:
                    print(f"  - {device['name']}")
                sys.exit(1)
        else:
            cast, browser = find_chromecast(args.speaker, devices)
            if not cast:
                print(f"Chromecast '{args.speaker}' not found.")
                sys.exit(1)
        # Get the actual device name from devices
        for device in devices:
            if device['name'].lower() == args.speaker.lower():
                cast_name = device['name']
                break
        else:
            cast_name = args.speaker
    else:
        # Use first available
        if not devices:
            print("No Chromecasts found!")
            sys.exit(1)

        if browser:
            cast = find_chromecast(devices[0]['name'], devices, browser)
        else:
            cast, browser = find_chromecast(devices[0]['name'], devices)
        cast_name = devices[0]['name']

    # Stream or play the audio
    if audio_file:
        # File or piped input
        play_to_chromecast(audio_file, cast, cast_name)
    else:
        # Live streaming for microphone input
        stream_audio_live(cast, cast_name)

finally:
    # Cleanup

    if browser:
        pychromecast.discovery.stop_discovery(browser)
    if temp_file and audio_file and os.path.exists(audio_file):
        os.unlink(audio_file)
    print("Done!")